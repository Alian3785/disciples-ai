import numpy as np
import gym
from gym import spaces
from stable_baselines3 import PPO
from stable_baselines3 import DQN
from stable_baselines3 import A2C
from sb3_contrib import TRPO
from sb3_contrib import ARS 
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
import wandb
from wandb.integration.sb3 import WandbCallback


class GoLeftEnv(gym.Env):
  """
  Custom Environment that follows gym interface.
  This is a simple env where the agent must learn to go always left. 
  """
  # Because of google colab, we cannot implement the GUI ('human' render mode)
  metadata = {'render.modes': ['console']}
  # Define constants for clearer code  
  ATTACK = 0
  ATTACK2 = 1
  ATTACK3 = 2
  LEFT = 3
  RIGHT = 4 
  
  def __init__(self, grid_size=100):
    super(GoLeftEnv, self).__init__()

    # Size of the 1D-grid
    self.grid_size = grid_size
    # Initialize the agent at the right of the grid
    self.piratespower = 4
    self.ghost = 52
    self.firstpiratehealth = self.piratespower
    self.secondpiratehealth = self.piratespower
    self.thirdpiratehealth = self.piratespower
    self.positionx = 1
    self.positiony = 1
    self.destination = 8
    self.border1 = 10
    self.border2 = 0
    # Define action and observation space
    # They must be gym.spaces objects
    # Example when using discrete actions, we have two: left and right
    n_actions = 5
    self.action_space = spaces.Discrete(n_actions)
    # The observation will be the coordinate of the agent
    # this can be described both by Discrete and Box space
    self.observation_space = spaces.Box(low=-100, high=100,
                                        shape=(6,), dtype=np.float32)

  def reset(self):
    """
    Important: the observation must be a numpy array
    :return: (np.array) 
    """
    # Initialize the agent at the right of the grid
    #self.agent_pos = self.grid_size - 1
    self.piratespower = 4
    self.ghost = 52
    self.firstpiratehealth = self.piratespower
    self.secondpiratehealth = self.piratespower
    self.thirdpiratehealth = self.piratespower
    self.positionx = 1
    self.positiony = 1
    self.destination = 8
    self.border1 = 10
    self.border2 = 0
    # here we convert to float32 to make it more general (in case we want to use continuous actions)
    return np.array([self.ghost, self.firstpiratehealth, self.secondpiratehealth, self.thirdpiratehealth, self.positionx, self.positiony]).astype(np.float32)

  def step(self, action):
    if action == self.ATTACK:
      if self.positionx == self.destination:
        self.firstpiratehealth = self.firstpiratehealth - 2
      else:
        self.secondpiratehealth = self.secondpiratehealth
    elif action == self.ATTACK2:
      if self.positionx == self.destination:
        self.secondpiratehealth = self.secondpiratehealth - 2
      else:
        self.secondpiratehealth = self.secondpiratehealth
    elif action == self.ATTACK3:
      if self.positionx == self.destination:
        self.thirdpiratehealth = self.thirdpiratehealth - 2
      else:
        self.secondpiratehealth = self.secondpiratehealth
    elif action == self.LEFT:
      if self.positionx == self.destination:        
        self.positionx = self.positionx - 0
      elif self.positionx == self.border2:
        self.positionx = self.positionx - 0
      else: 
        self.positionx = self.positionx - 1
    elif action == self.RIGHT:
      if self.positionx == self.destination:        
        self.positionx = self.positionx + 0
      elif self.positionx == self.border1:
        self.positionx = self.positionx + 0
      else: 
        self.positionx = self.positionx + 1     
    else:
      raise ValueError("Received invalid action={} which is not part of the action space".format(action))

    if self.firstpiratehealth > 0 and self.positionx == self.destination:
      self.ghost = self.ghost - 2 
    else:
      self.ghost = self.ghost 

    if self.secondpiratehealth > 0 and self.positionx == self.destination:
      self.ghost = self.ghost - 2 
    else:
      self.ghost = self.ghost 

    if self.thirdpiratehealth > 0 and self.positionx == self.destination:
      self.ghost = self.ghost - 2 
    else:
      self.ghost = self.ghost 


    done = bool((self.secondpiratehealth <= 0 and self.firstpiratehealth <= 0 and self.thirdpiratehealth <= 0) or self.ghost <= 0) 


    if (self.secondpiratehealth <= 0 and self.firstpiratehealth <= 0 and self.thirdpiratehealth <= 0 and self.ghost > 0):
      reward = 100 
    else:
      reward = 0

    # Optionally we can pass additional info, we are not using that for now
    info = {}

    return np.array([self.ghost, self.firstpiratehealth, self.secondpiratehealth, self.thirdpiratehealth, self.positionx, self.positiony]).astype(np.float32), reward, done, info

  def render(self, mode='console'):
    print("Позиция 1 - ")
    print(self.positionx)
    print("Здоровье призрака - ")
    print(self.ghost)
    print("Здоровье первого пирата - ")
    print(self.firstpiratehealth)
    print("Здоровье второго пирата - ")
    print(self.secondpiratehealth)
    print("Здоровье третьего пирата - ")
    print(self.thirdpiratehealth)
    if mode != 'console':
      raise NotImplementedError()

  def close(self):
    pass

from stable_baselines3.common.env_util import make_vec_env

# Instantiate the env
env = GoLeftEnv(grid_size=10)
# wrap it
env = make_vec_env(lambda: env, n_envs=1)

# Train the agent

config = {
    "policy": 'MlpPolicy',
    "total_timesteps": 40000
}

wandb.init(
    config=config,
    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B
    project="goturnbased",
  #  monitor_gym=True,       # automatically upload gym environements' videos
  # save_code=True,
)

model = PPO(config['policy'], env, verbose=1, tensorboard_log=f"runs/ppo")
model.learn(total_timesteps=config['total_timesteps'])
wandb.finish()
    
